{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mhkim/data/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mhkim/data/mnist/train-labels-idx1-ubyte.gz\nExtracting /home/mhkim/data/mnist/t10k-images-idx3-ubyte.gz\nExtracting /home/mhkim/data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\nWARNING:tensorflow:From <ipython-input-1-12b140a1fe3b>:279 in main.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\nInstructions for updating:\nPlease switch to tf.summary.merge_all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-12b140a1fe3b>:280 in main.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\nInstructions for updating:\nPlease switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 0 (epoch 0.00), 9.8 ms\nMinibatch loss: 8.334, learning rate: 0.010000\nMinibatch error: 85.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 84.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 100 (epoch 0.12), 159.1 ms\nMinibatch loss: 3.250, learning rate: 0.010000\nMinibatch error: 6.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 7.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 200 (epoch 0.23), 150.5 ms\nMinibatch loss: 3.377, learning rate: 0.010000\nMinibatch error: 12.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 4.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 300 (epoch 0.35), 149.9 ms\nMinibatch loss: 3.176, learning rate: 0.010000\nMinibatch error: 7.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 3.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 400 (epoch 0.47), 148.0 ms\nMinibatch loss: 3.216, learning rate: 0.010000\nMinibatch error: 7.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 2.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 500 (epoch 0.58), 147.9 ms\nMinibatch loss: 3.179, learning rate: 0.010000\nMinibatch error: 6.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 2.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 600 (epoch 0.70), 148.1 ms\nMinibatch loss: 3.130, learning rate: 0.010000\nMinibatch error: 3.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 2.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 700 (epoch 0.81), 147.6 ms\nMinibatch loss: 2.984, learning rate: 0.010000\nMinibatch error: 4.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 2.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 800 (epoch 0.93), 148.3 ms\nMinibatch loss: 3.040, learning rate: 0.010000\nMinibatch error: 7.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 2.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 900 (epoch 1.05), 146.3 ms\nMinibatch loss: 2.907, learning rate: 0.009500\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 1000 (epoch 1.16), 147.6 ms\nMinibatch loss: 2.859, learning rate: 0.009500\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 2.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 1100 (epoch 1.28), 146.1 ms\nMinibatch loss: 2.812, learning rate: 0.009500\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 1200 (epoch 1.40), 147.3 ms\nMinibatch loss: 2.911, learning rate: 0.009500\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 1300 (epoch 1.51), 152.7 ms\nMinibatch loss: 2.819, learning rate: 0.009500\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 1400 (epoch 1.63), 146.9 ms\nMinibatch loss: 2.826, learning rate: 0.009500\nMinibatch error: 3.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 1500 (epoch 1.75), 148.5 ms\nMinibatch loss: 2.905, learning rate: 0.009500\nMinibatch error: 4.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 1600 (epoch 1.86), 149.9 ms\nMinibatch loss: 2.731, learning rate: 0.009500\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 1700 (epoch 1.98), 147.1 ms\nMinibatch loss: 2.659, learning rate: 0.009500\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 1800 (epoch 2.09), 147.5 ms\nMinibatch loss: 2.660, learning rate: 0.009025\nMinibatch error: 3.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 1900 (epoch 2.21), 147.3 ms\nMinibatch loss: 2.636, learning rate: 0.009025\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 2000 (epoch 2.33), 147.4 ms\nMinibatch loss: 2.619, learning rate: 0.009025\nMinibatch error: 3.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 2100 (epoch 2.44), 147.6 ms\nMinibatch loss: 2.568, learning rate: 0.009025\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 2200 (epoch 2.56), 150.9 ms\nMinibatch loss: 2.572, learning rate: 0.009025\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 2300 (epoch 2.68), 146.5 ms\nMinibatch loss: 2.593, learning rate: 0.009025\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 2400 (epoch 2.79), 146.0 ms\nMinibatch loss: 2.503, learning rate: 0.009025\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 2500 (epoch 2.91), 147.0 ms\nMinibatch loss: 2.467, learning rate: 0.009025\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 2600 (epoch 3.03), 146.4 ms\nMinibatch loss: 2.469, learning rate: 0.008574\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 2700 (epoch 3.14), 146.7 ms\nMinibatch loss: 2.493, learning rate: 0.008574\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 2800 (epoch 3.26), 148.8 ms\nMinibatch loss: 2.456, learning rate: 0.008574\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 2900 (epoch 3.37), 149.5 ms\nMinibatch loss: 2.487, learning rate: 0.008574\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 3000 (epoch 3.49), 146.3 ms\nMinibatch loss: 2.389, learning rate: 0.008574\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 3100 (epoch 3.61), 148.8 ms\nMinibatch loss: 2.370, learning rate: 0.008574\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 3200 (epoch 3.72), 150.8 ms\nMinibatch loss: 2.338, learning rate: 0.008574\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 3300 (epoch 3.84), 146.3 ms\nMinibatch loss: 2.330, learning rate: 0.008574\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 3400 (epoch 3.96), 146.5 ms\nMinibatch loss: 2.288, learning rate: 0.008574\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 3500 (epoch 4.07), 146.4 ms\nMinibatch loss: 2.282, learning rate: 0.008145\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 3600 (epoch 4.19), 145.9 ms\nMinibatch loss: 2.250, learning rate: 0.008145\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 3700 (epoch 4.31), 148.8 ms\nMinibatch loss: 2.228, learning rate: 0.008145\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 3800 (epoch 4.42), 148.0 ms\nMinibatch loss: 2.215, learning rate: 0.008145\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 3900 (epoch 4.54), 146.7 ms\nMinibatch loss: 2.218, learning rate: 0.008145\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 4000 (epoch 4.65), 147.6 ms\nMinibatch loss: 2.259, learning rate: 0.008145\nMinibatch error: 3.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 4100 (epoch 4.77), 149.7 ms\nMinibatch loss: 2.171, learning rate: 0.008145\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 4200 (epoch 4.89), 147.2 ms\nMinibatch loss: 2.173, learning rate: 0.008145\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 4300 (epoch 5.00), 146.6 ms\nMinibatch loss: 2.203, learning rate: 0.007738\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 4400 (epoch 5.12), 150.6 ms\nMinibatch loss: 2.130, learning rate: 0.007738\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 4500 (epoch 5.24), 147.1 ms\nMinibatch loss: 2.201, learning rate: 0.007738\nMinibatch error: 4.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 4600 (epoch 5.35), 148.0 ms\nMinibatch loss: 2.091, learning rate: 0.007738\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 4700 (epoch 5.47), 152.2 ms\nMinibatch loss: 2.103, learning rate: 0.007738\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 4800 (epoch 5.59), 146.2 ms\nMinibatch loss: 2.065, learning rate: 0.007738\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 4900 (epoch 5.70), 147.7 ms\nMinibatch loss: 2.081, learning rate: 0.007738\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 5000 (epoch 5.82), 151.9 ms\nMinibatch loss: 2.114, learning rate: 0.007738\nMinibatch error: 4.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 5100 (epoch 5.93), 146.4 ms\nMinibatch loss: 2.004, learning rate: 0.007738\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 5200 (epoch 6.05), 146.4 ms\nMinibatch loss: 2.081, learning rate: 0.007351\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 5300 (epoch 6.17), 150.1 ms\nMinibatch loss: 1.971, learning rate: 0.007351\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 5400 (epoch 6.28), 148.0 ms\nMinibatch loss: 1.959, learning rate: 0.007351\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 5500 (epoch 6.40), 148.1 ms\nMinibatch loss: 1.979, learning rate: 0.007351\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 5600 (epoch 6.52), 150.4 ms\nMinibatch loss: 1.940, learning rate: 0.007351\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 5700 (epoch 6.63), 146.9 ms\nMinibatch loss: 1.914, learning rate: 0.007351\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 5800 (epoch 6.75), 148.1 ms\nMinibatch loss: 1.898, learning rate: 0.007351\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 5900 (epoch 6.87), 150.1 ms\nMinibatch loss: 1.890, learning rate: 0.007351\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 6000 (epoch 6.98), 147.6 ms\nMinibatch loss: 1.893, learning rate: 0.007351\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 6100 (epoch 7.10), 146.6 ms\nMinibatch loss: 1.865, learning rate: 0.006983\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 6200 (epoch 7.21), 149.8 ms\nMinibatch loss: 1.843, learning rate: 0.006983\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 6300 (epoch 7.33), 147.4 ms\nMinibatch loss: 1.837, learning rate: 0.006983\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 6400 (epoch 7.45), 146.5 ms\nMinibatch loss: 1.829, learning rate: 0.006983\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 6500 (epoch 7.56), 147.2 ms\nMinibatch loss: 1.807, learning rate: 0.006983\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 6600 (epoch 7.68), 147.2 ms\nMinibatch loss: 1.808, learning rate: 0.006983\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 6700 (epoch 7.80), 146.6 ms\nMinibatch loss: 1.784, learning rate: 0.006983\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 6800 (epoch 7.91), 146.4 ms\nMinibatch loss: 1.779, learning rate: 0.006983\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 6900 (epoch 8.03), 149.0 ms\nMinibatch loss: 1.759, learning rate: 0.006634\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 7000 (epoch 8.15), 147.7 ms\nMinibatch loss: 1.756, learning rate: 0.006634\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 7100 (epoch 8.26), 148.3 ms\nMinibatch loss: 1.736, learning rate: 0.006634\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 7200 (epoch 8.38), 145.5 ms\nMinibatch loss: 1.740, learning rate: 0.006634\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 7300 (epoch 8.49), 145.6 ms\nMinibatch loss: 1.727, learning rate: 0.006634\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 7400 (epoch 8.61), 147.1 ms\nMinibatch loss: 1.700, learning rate: 0.006634\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 7500 (epoch 8.73), 145.3 ms\nMinibatch loss: 1.695, learning rate: 0.006634\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 7600 (epoch 8.84), 145.6 ms\nMinibatch loss: 1.735, learning rate: 0.006634\nMinibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 7700 (epoch 8.96), 146.5 ms\nMinibatch loss: 1.666, learning rate: 0.006634\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 7800 (epoch 9.08), 152.8 ms\nMinibatch loss: 1.659, learning rate: 0.006302\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 7900 (epoch 9.19), 147.1 ms\nMinibatch loss: 1.648, learning rate: 0.006302\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 8000 (epoch 9.31), 146.1 ms\nMinibatch loss: 1.642, learning rate: 0.006302\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 8100 (epoch 9.43), 150.3 ms\nMinibatch loss: 1.630, learning rate: 0.006302\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 8200 (epoch 9.54), 149.9 ms\nMinibatch loss: 1.622, learning rate: 0.006302\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 8300 (epoch 9.66), 146.5 ms\nMinibatch loss: 1.612, learning rate: 0.006302\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 8400 (epoch 9.77), 149.5 ms\nMinibatch loss: 1.596, learning rate: 0.006302\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\nStep 8500 (epoch 9.89), 145.7 ms\nMinibatch loss: 1.618, learning rate: 0.006302\nMinibatch error: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 0.8%\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Simple, end-to-end, LeNet-5-like convolutional MNIST model example.\n",
    "\n",
    "This should achieve a test error of 0.7%. Please keep this model as simple and\n",
    "linear as possible, it is meant as a tutorial for simple convolutional models.\n",
    "Run with --self_test on the command line to execute a short self-test.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "WORK_DIRECTORY = '/home/mhkim/data/mnist'\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100  # Number of steps between evaluations.\n",
    "\n",
    "train_checkpoint = '/home/mhkim/data/convolution_train_cpu/'\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def data_type():\n",
    "  \"\"\"Return the type of the activations, weights, and placeholder variables.\"\"\"\n",
    "  if FLAGS.use_fp16:\n",
    "    return tf.float16\n",
    "  else:\n",
    "    return tf.float32\n",
    "\n",
    "\n",
    "def maybe_download(filename):\n",
    "  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "  if not tf.gfile.Exists(WORK_DIRECTORY):\n",
    "    tf.gfile.MakeDirs(WORK_DIRECTORY)\n",
    "  filepath = os.path.join(WORK_DIRECTORY, filename)\n",
    "  if not tf.gfile.Exists(filepath):\n",
    "    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "    with tf.gfile.GFile(filepath) as f:\n",
    "      size = f.size()\n",
    "    print('Successfully downloaded', filename, size, 'bytes.')\n",
    "  return filepath\n",
    "\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "\n",
    "  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "  \"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(16)\n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
    "    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n",
    "    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(8)\n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n",
    "  return labels\n",
    "\n",
    "\n",
    "def fake_data(num_images):\n",
    "  \"\"\"Generate a fake dataset that matches the dimensions of MNIST.\"\"\"\n",
    "  data = numpy.ndarray(\n",
    "      shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS),\n",
    "      dtype=numpy.float32)\n",
    "  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)\n",
    "  for image in xrange(num_images):\n",
    "    label = image % 2\n",
    "    data[image, :, :, 0] = label - 0.5\n",
    "    labels[image] = label\n",
    "  return data, labels\n",
    "\n",
    "\n",
    "def error_rate(predictions, labels):\n",
    "  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n",
    "  return 100.0 - ( 100.0 * numpy.sum(numpy.argmax(predictions, 1) == labels) / predictions.shape[0])\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if FLAGS.self_test:\n",
    "    print('Running self-test.')\n",
    "    train_data, train_labels = fake_data(256)\n",
    "    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\n",
    "    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\n",
    "    num_epochs = 1\n",
    "  else:\n",
    "    # Get the data.\n",
    "    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # Extract it into numpy arrays.\n",
    "    train_data = extract_data(train_data_filename, 60000)\n",
    "    train_labels = extract_labels(train_labels_filename, 60000)\n",
    "\n",
    "    test_data = extract_data(test_data_filename, 10000)\n",
    "    test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "    # Generate a validation set.\n",
    "    validation_data = train_data[:VALIDATION_SIZE, ...]\n",
    "    validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "    train_data = train_data[VALIDATION_SIZE:, ...]\n",
    "    train_labels = train_labels[VALIDATION_SIZE:]\n",
    "    num_epochs = NUM_EPOCHS\n",
    "\n",
    "  #print(test_data)\n",
    "  #print(test_labels)\n",
    "\n",
    "\n",
    "\n",
    "  train_size = train_labels.shape[0]\n",
    "\n",
    "  # This is where training samples and labels are fed to the graph.\n",
    "  # These placeholder nodes will be fed a batch of training data at each\n",
    "  # training step using the {feed_dict} argument to the Run() call below.\n",
    "  train_data_node = tf.placeholder( data_type(), shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS) , name=\"train_data_node\")\n",
    "  train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,), name=\"train_labels_node\")\n",
    "\n",
    "  eval_data = tf.placeholder( data_type(), shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS), name='eval_data')\n",
    "\n",
    "  # The variables below hold all the trainable weights. They are passed an\n",
    "  # initial value which will be assigned when we call:\n",
    "  # {tf.global_variables_initializer().run()}\n",
    "  conv1_weights = tf.Variable(tf.truncated_normal([5, 5, NUM_CHANNELS, 32],stddev=0.1,seed=SEED, dtype=data_type()), name='conv1_weights')\n",
    "  conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type()), name='conv1_biases')\n",
    "  # 5x5 filter, depth 32.\n",
    "\n",
    "  conv2_weights = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1, seed=SEED, dtype=data_type()), name='conv2_weights')\n",
    "  conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type()), name='conv2_biases')\n",
    "\n",
    "  # fully connected, depth 512.\n",
    "  fc1_weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512], stddev=0.1, seed=SEED, dtype=data_type()), name='fc1_weights')\n",
    "  fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type()), name='fc1_biases')\n",
    "\n",
    "  fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS], stddev=0.1, seed=SEED, dtype=data_type()), name='fc2_weights')\n",
    "  fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype=data_type()), name='fc2_biases')\n",
    "\n",
    "  # We will replicate the model structure for the training subgraph, as well\n",
    "  # as the evaluation subgraphs, while sharing the trainable parameters.\n",
    "  def model(data, train=False, name='model'):\n",
    "    with tf.name_scope(name) :\n",
    "        \"\"\"The Model definition.\"\"\"\n",
    "        # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "        # the same size as the input). Note that {strides} is a 4D array whose\n",
    "        # shape matches the data layout: [image index, y, x, depth].\n",
    "        conv = tf.nn.conv2d(data, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        # Bias and rectified linear non-linearity.\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "        # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "        # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        conv = tf.nn.conv2d(pool, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "        # fully connected layers.\n",
    "        pool_shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "        # Fully connected layer. Note that the '+' operation automatically\n",
    "        # broadcasts the biases.\n",
    "\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "        # Add a 50% dropout during training only. Dropout also scales\n",
    "        # activations such that no rescaling is needed at evaluation time.\n",
    "        if train:\n",
    "          hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "\n",
    "        return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "  # Training computation: logits + cross-entropy loss.\n",
    "  logits = model(train_data_node, True, 'train')\n",
    "\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits , labels=train_labels_node))\n",
    "\n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                  tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 5e-4 * regularizers\n",
    "\n",
    "  # Optimizer: set up a variable that's incremented once per batch and\n",
    "  # controls the learning rate decay.\n",
    "  batch = tf.Variable(0, dtype=data_type(), name='batch')\n",
    "  # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "      0.01,                # Base learning rate.\n",
    "      batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "      train_size,          # Decay step.\n",
    "      0.95,                # Decay rate.\n",
    "      staircase=True)\n",
    "  # Use simple momentum for the optimization.\n",
    "  optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer (learning_rate).minimize(loss, global_step=batch)\n",
    "\n",
    "  #optimizer = tf.train.AdamOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "\n",
    "  # Predictions for the current training minibatch.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Predictions for the test and validation, which we'll compute less often.\n",
    "  eval_prediction = tf.nn.softmax(model(eval_data, name='eval'))\n",
    "\n",
    "  # Small utility function to evaluate a dataset by feeding batches of data to\n",
    "  # {eval_data} and pulling the results from {eval_predictions}.\n",
    "  # Saves memory and enables this to run on smaller GPUs.\n",
    "  def eval_in_batches(data, sess):\n",
    "    \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "      raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "\n",
    "    predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n",
    "    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "      end = begin + EVAL_BATCH_SIZE\n",
    "      if end <= size:\n",
    "        predictions[begin:end, :] = sess.run( eval_prediction, feed_dict={eval_data: data[begin:end, ...]})\n",
    "      else:\n",
    "        batch_predictions = sess.run( eval_prediction, feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "        predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions\n",
    "\n",
    "  # Create a local session to run the training.\n",
    "  start_time = time.time()\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    # Run all the initializers to prepare the trainable parameters.\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    print('Initialized!')\n",
    "\n",
    "    merged = tf.merge_all_summaries()\n",
    "    writer = tf.train.SummaryWriter(\"/tmp/tensorflow-convolution-cpu\", sess.graph)\n",
    "\n",
    "    saver.save(sess=sess, save_path=os.path.join(train_checkpoint, 'convolution-cpu.ckpt'))\n",
    "\n",
    "    # Loop through training steps.\n",
    "    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n",
    "      # Compute the offset of the current minibatch in the data.\n",
    "      # Note that we could use better randomization across epochs.\n",
    "      offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "      batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "      batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "      # This dictionary maps the batch data (as a numpy array) to the\n",
    "      # node in the graph it should be fed to.\n",
    "      feed_dict = { train_data_node: batch_data, train_labels_node: batch_labels }\n",
    "\n",
    "      # Run the optimizer to update weights.\n",
    "      sess.run(optimizer, feed_dict=feed_dict)\n",
    "      # print some extra information once reach the evaluation frequency\n",
    "\n",
    "      if step % EVAL_FREQUENCY == 0:\n",
    "        # fetch some extra nodes' data\n",
    "        l, lr, predictions = sess.run([loss, learning_rate, train_prediction], feed_dict=feed_dict)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "\n",
    "        print('--------------------------------------------------------------------')\n",
    "\n",
    "        print('Step %d (epoch %.2f), %.1f ms' % (step, float(step) * BATCH_SIZE / train_size, 1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "        print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "        print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n",
    "        print('Validation error: %.1f%%' % error_rate(eval_in_batches(validation_data, sess), validation_labels))\n",
    "\n",
    "        '''\n",
    "        print('Step %d (epoch %.2f), %.1f ms, error rate : %.1f%%' % (step, float(step) * BATCH_SIZE / train_size\n",
    "                                                 , 1000 * elapsed_time / EVAL_FREQUENCY\n",
    "                                                , error_rate(eval_in_batches(validation_data, sess), validation_labels)))\n",
    "        '''\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n",
    "\n",
    "    print('Test error: %.1f%%' % test_error)\n",
    "\n",
    "    if FLAGS.self_test:\n",
    "      print('test_error', test_error)\n",
    "      assert test_error == 0.0, 'expected 0.0 test_error, got %.2f' % ( test_error,)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--use_fp16',\n",
    "      default=False,\n",
    "      help='Use half floats instead of full floats if True.',\n",
    "      action='store_true')\n",
    "  parser.add_argument(\n",
    "      '--self_test',\n",
    "      default=False,\n",
    "      action='store_true',\n",
    "      help='True if running a self test.')\n",
    "\n",
    "  if tf.gfile.Exists(train_checkpoint):\n",
    "    tf.gfile.DeleteRecursively(train_checkpoint)\n",
    "  tf.gfile.MakeDirs(train_checkpoint)\n",
    "\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}